---
layout: post
comments: true
title: Table Structure Recognition
author: Ankur Kumar, Pranay Shirodkar
date: 2022-04-10
---


> In this work, we will explore ways to improve latex table structure recognition. The task involves generating latex code for input table image. This will be useful in making latex more accessible to everyone. We also get to understand some of the challenges involved in applying deep learning techniques, normally developed for natural images, to a different domain, here table images.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction

## Related work

## Dataset
We use dataset from ICDAR 2021 challenge for table structure recognition. It provides three splits: training, validation and test sets. Training set contains around 43K image-code pairs. Validation and test sets do not contain target code sequences. Therefore, we split training set into 90:10 ratio to create train and validation set for our experiments. We use validaiton set to adapt learning rate schedule and for final testing of the model performance. We use word-level tokenization for the target sequence due to closed  and small vocabulary. There are only 30 tokens/words in the final vocabulary. The input image has fixed resolution of 400x400. Some sample images from traning set are shown below. We find that fixing the resolution severly distorts many images and there it may not be ideal preprocessing. Also, the dataset is not high quality dataset. We observe some label noise as well. Specifically, many target sequences don't compile. We evaluate our models using word-level accuracy and sentence accuracy.
<!-- training samples -->

## Our Approach
We first discuss the baseline model. Then we discuss several improvements that we tried to improve over the baseline accuracy. Our results are summarized in the results section.

### Baseline
Given an input image containing latex table with data, the task is to generate corresponding latex code. This problem can be thought of as image captioning task and can be tackled using classic encoder-decoder framework. We first use ResNet18 as image encoder. Using transformer encoder layers on top of this, similar to scene text recognition in [3], did not improve. So we left out this for the remainder of experiments. Our baseline model has 3 transformer layers in decoder and all but the last block of ResNet18 as encoder. The downsampling is 16x. Input image size is 400x400. Therefore, decoder input has 25x25=625 tokens. We also tried to remove the pooling layer in ResNet18 but it degraded the accuracy possibly because this significantly increases the number of tokens to 50x50=2500. SER for the baseline model trained for 10 epochs with a learning rate of 0.0001 and AdamW optimizer is 22%.
<!-- baseline model architecture -->

### Pre-training Decoder
A common technique is to initialize decoder weigths with LM trained on similar task. We use ground truth target sequences in training data to train transformer LM and use it to initialize decoder weights. The pre-training is done for 10 epochs. We transfer the LM weights to decoder and train for 10 epochs which results in more than 10% increase in sentence accuracy.

### Pre-train Encoder
How do we pre-train image encoder for table structure recognition? We only have access to table images. Non-contrastive and generative self-supervised methods can be used. However, the former makes use of two different views of same image and minimizes distance between them. The problem with this approach for our task is that two random crops from same table image do not have any semantic similarity. Therefore, we use a different approach to obtain image pairs with same table structure. We compile the ground truth target sequences to obtain dummy tables. These tables differ from original tables in term of content only as shown below. The newly generated image and its original image can be considered as two different views. Now we can use the non-contrastive approaches. We do not work experiment with contrastive methods due to their requirement of large batch size for hard negative mining. We discuss all the pre-training methods below.
<!-- add original to dummy table image from slide -->

#### Curriculum Learning
We hypothesized that dummy table images are easy examples of table images due to simplicity of table content. This is in contrast with original table images whose content is not relevant for the task but can vary a lot. Therefore, we can leverage this in curriculum learning framework by first training the entire model end-to-end using dummy table images and ground truth sequences. Then the learned weights are trained again on original table images and target sequences. However, it does not  work since the accuracy of the final model is same as in the case where image encoder is randomly initialized before training on original dataset. The reason can be the difference in input data distribution. It would be interesting to see if the exact opposite holds, i.e. does pre-training the network on the original images improve structure recognition of dummy table images?

#### Non-contrastive Pre-training with SimSiam
Although contrastive/non-contrastive pre-training heavily depends on image augmentation, we still experiment with one such approach despite limited data augmentation available for our task. We explore SimSiam [3] due to its simple and effective approach in unsupervised representation learning.  As discussed earlier, we can use dummy table and corresponding original table images as two views. We try to minimize their distance in some latent space as shown below. The network is trained with negative cosine similarity loss. However, we find that the loss reaches around minimum value of -1 after few epochs indicating that the image encoder collapses to a degenarate solution. There could be mutliple reasons for this: limited dataset, no augmentation, inherent difficulty of task. We found other users facing similar challenges when pre-training on different datasets.
<!-- add image for simsiam from slide -->

#### Generative Pre-training with MAE
We can use masked autoencoder (MAE) [5] for pre-training. Conceptually, algorithms based on MAE should not be useful for our task despite being able to successfully reconstruct the table structure (content may be difficult to reconstruct with limited data). The reason being MAE makes the assumption that the masked regions can be filled using global knowledge of what object is present in the image as well as local knowledge of color etc. Here, we want the model to learn global semantics of table structure. But there is no way to force the model to learn to do so because table structure, i.e. horizontal and vertical lines can be filled in the masked region with local knowledge itself. The content of the table is difficult to reconstruct and even if it can be done, local information may be more useful. We observe these in the experiments also. If we simplify the table content by using dummy table images, we find that the model is able to almost prefectly reconstruct the input image as shown below. Similarly with the original images, model is able to reconstruct the horizontal and vertical lines. However, it does not bring any performance gain for our task.
<!-- add image for MAE reconstruction from slide -->

#### Pix2Pix Style Generative Pre-training with ConvMAE
We can force the model to reconstruct the dummy image given original table image as input. This should help to learn the global table structure present in the original image. Why does this make sense? This is because there is no straightforward relationship between pixels of the two images which can be exploited for this reconstruction task. The only way we can reconstruct the dummy table is by knowing the table structure of original image, i.e. focusing on rows and columns. This is in contrast with the previous approach. The caveat here is that model may find it difficult to reconstruct the dummy image perfectly with limited training as shown below. It is not clear if the model learns table structure or some spurious correlations.
<!-- add image for convMAE reconstruction from slide -->

We make an extra change. We use ConvMAE [6] instead of MAE because of its multi-scale hybrid convolution-transformer which has shown improved performance over the MAE architecture. In our experiments, we use the base configuration. It uses image size of 224 with a progressive downsampling of 16x. Therefore, we have around 32x downsampling if we compare with the original resolution of 400x400. We tried to use the original resolution but it increases per epoch training time from 30 minutes to around 2 hours, which makes training for even 10 epochs impractical. There are 15 encoder blocks and 8 decoder blocks. Encoder transformer blocks use 12 attention heads whereas decoder transformer blocks use 16 heads. More details about the architecture can be found at the official github implementation [7].
<!-- add convmae architecture figure 1 from convmae paper -->

We pre-train the image encoder for 40 epochs. For finetuning, we use three variations mainly in the decoder block: 1. 8 pre-trained decoder blocks 2. 4 pre-trained decoder blocks 3. 4 pre-trained + 4 randomly initialized decoder blocks. The entire network containes more than 100M parameters and is trained with learning rate of 0.0001 for 20 epochs. We find that all three variations acheive same validation loss after 20 epochs but the first variation has lower sentence accuracy than the other two cases. Nonetheless, this pre-training performs much better than the other two self-supervised pre-training strategies discussed above. It has accuracy very close to the baseline. However, it is difficult to draw conclusion due to different encoder architectures and model sizes.

### Results
| Model       | Validation Loss    |  Word Accuracy     | Sentence Accuracy |
| :---        |    :----:   |         :---: | :---: |
| Baseline (ResNet18 + 3 transformer decoder layers)        | ---        | ---          | 22% |
| + Decoder Pre-training        | ---        | ---          | 32% |
| + 10 epochs        | 0.076        | 92.1%          | 49.1% |
| MAE        | 0.140        | ---          | --- |
| ConvMAE (8 pre-trained decoder blocks)        | 0.107        | 90.9%          | 44.1% |
| ConvMAE (4 pre-trained decoder blocks)        | 0.109        | 92.6%          | 48.7% |
| ConvMAE (4 pre-trained + 4 randomly initialized decoder blocks)        | 0.107        | 91.9%          | 48.2% |

### Demo
Our best performing model is hosted [here](https://huggingface.co/spaces/iankur/img2tex) on Huggingface spaces. Interested readers can navigate to the page and input desired tables to see model's outputs.

### Error Analysis
We find that the model makes mistake on white spaces bordering the input image. This is because the training data contains tightly cropped images. Removing the white space corrects some of the issues. Multicolumn enteries are also hard to predict correctly. Then there is horizontal ruling which is skipped some times.

### Future Work

## References
[1] Kayal, Pratik et al. “ICDAR 2021 Competition on Scientific Table Image Recognition to LaTeX.” ArXiv abs/2105.14426 (2021)

[2] He, Yelin et al. “PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Table Image Recognition to Latex.” ArXiv abs/2105.01846 (2021)

[3] Feng, Xinjie et al. “Scene Text Recognition via Transformer.” ArXiv abs/2003.08077 (2020)

[4] Chen, Xinlei and Kaiming He. “Exploring Simple Siamese Representation Learning.” 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)

[5] He, Kaiming et al. “Masked Autoencoders Are Scalable Vision Learners.” ArXiv abs/2111.06377 (2021)

[6] Gao, Peng et al. “ConvMAE: Masked Convolution Meets Masked Autoencoders.” ArXiv abs/2205.03892 (2022)

[7] https://github.com/Alpha-VL/ConvMAE

---
